{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables de entorno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_type = os.environ[\"OPENAI_API_TYPE\"]\n",
    "openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vista general de los LLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los LLM son modelos de aprendizaje profundo (deep learning) con miles de millones de parámetros que destacan en una amplia gama de tareas de procesamiento del lenguaje natural. Pueden realizar tareas como la traducción, el análisis de sentimientos y las conversaciones de chatbot sin haber sido entrenados específicamente para ello. Los LLM pueden utilizarse sin necesidad de un fine-tuning empleando técnicas de \"prompting\", en las que una pregunta se presenta como un prompt de texto con ejemplos de problemas y soluciones similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Arquitectura:\n",
    "\n",
    "Los LLM constan de varias capas de redes neuronales, capas feedforward, capas de embeddings y capas de atención. Estas capas trabajan juntas para procesar el texto de entrada y generar predicciones de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implicaciones futuras:\n",
    "\n",
    "Aunque los LLM tienen el potencial de revolucionar diversos sectores, es importante ser conscientes de sus limitaciones e implicaciones éticas. Las empresas y los trabajadores deben considerar detenidamente las compensaciones y los riesgos asociados al uso de los LLM, y los desarrolladores deben seguir perfeccionando estos modelos para minimizar los sesgos y mejorar su utilidad en diferentes aplicaciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, veremos una de las principales limitaciones a tener en cuenta a la hora de utilizar modelos de lenguaje, y es una variable que depende de cada implementación en específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Máximo número de tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tamaño del contexto del LLM, o el número máximo de tokens que el modelo puede procesar, viene determinado por la implementación específica del LLM. En el caso de la implementación de OpenAI en LangChain, el número máximo de tokens viene definido por el modelo OpenAI que se esté utilizando. Para encontrar el número máximo de tokens para el modelo OpenAI, consulte el atributo max_tokens proporcionado en la documentación o API de OpenAI. \n",
    "\n",
    "OpenAI API Doc: https://platform.openai.com/docs/models/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, si estás usando el modelo GPT-3, el número máximo de tokens que es soportado por el modelo son **2049** tokens. La máxima cantidad de tokens también depende de la versión específica lanzada del modelo (por ejemplo, davinci, curie, babbage o ada). Cada versión tiene diferentes limitaciones, normalmente las versiones más recientes son las que más número de tokens soportan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante asegurarse de que el texto de entrada no supera el número máximo de tokens admitidos por el modelo, ya que esto puede provocar errores durante el procesamiento. Para ello, puede dividir el texto de entrada en trozos más pequeños y procesarlos por separado (chunks), asegurándose de que cada chunk está dentro del límite de tokens permitido. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo de cómo tratar un texto que excede el límite máximo de tokens para un determinado LLM utilizando LangChain. Tenga en cuenta que el siguiente código es en parte pseudocódigo. No se supone que se ejecute, pero debería darte una idea de cómo manejar textos más largos que el límite máximo de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Intenta es', 'cribir un ', 'input de t', 'exto muy l', 'argo para ', 'procesar']\n",
      "Intenta escribir un input de texto muy largo para procesar\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "Funciones de utilidad para procesas texto y tokens en LLMs\n",
    "\"\"\"\n",
    "def split_text_into_chunks(input_text, max_tokens):\n",
    "    chunks = len(input_text) // max_tokens    \n",
    "    if (len(input_text) > max_tokens) and (len(input_text)%max_tokens != 0):\n",
    "        process_chunks = [input_text[max_tokens*char:(char+1)*max_tokens] for char in range(chunks)] \n",
    "        process_chunks.append(input_text[max_tokens*chunks:len(input_text)])\n",
    "        return process_chunks\n",
    "    elif (len(input_text) > max_tokens) and (len(input_text)%max_tokens==0):\n",
    "        return [input_text[max_tokens*char:(char+1)*max_tokens] for char in range(chunks)]\n",
    "    return input_text\n",
    "\n",
    "def combine_results(chunk_list):\n",
    "    return \"\".join(chunk_list)\n",
    "\n",
    "input_text = \"Intenta escribir un input de texto muy largo para procesar\"\n",
    "max_tokens = 10\n",
    "print(split_text_into_chunks(input_text, max_tokens))\n",
    "print(combine_results(split_text_into_chunks(input_text, max_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Antes de ejecutar el código, asegurar tener la OpenAI key\n",
    "# en las variables de entorno \"OPENAI_API_KEY\"\n",
    "# Instanciar el LLM\n",
    "llm = OpenAI(engine=\"text-davinci-003\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"Intenta escribir un input de texto largo para procesar\"\n",
    "\n",
    "# Determinamos el número máximo de tokens permitidos según la doc de OpenAI\n",
    "max_tokens = 4097\n",
    "\n",
    "# Divide el input de texto en chunks según el máximo de tokens\n",
    "text_chunks = split_text_into_chunks(input_text, max_tokens)\n",
    "\n",
    "# Procesar cada chunk de forma separada\n",
    "results = []\n",
    "for chunk in text_chunks:\n",
    "    result = llm(chunk)\n",
    "    results.append(result)\n",
    "\n",
    "# Combinamos los resultados\n",
    "final_result = combine_results(results)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribuciones de Tokens y prediciendo el siguiente Token**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Large Language models, como GPT-3 y GPT-4, se entrenan previamente con grandes cantidades de datos de texto y aprenden a predecir el siguiente token de una secuencia basándose en el contexto proporcionado por los tokens anteriores. Los modelos de la familia GPT utilizan el modelado causal del lenguaje, que predice el siguiente token teniendo sólo acceso a los tokens anteriores. Este proceso permite a los LLM generar texto contextualmente relevante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código utiliza la clase OpenAI de LangChain para cargar la variación Davinci de GPT-3 utilizando la key text-davinci-003 para completar la secuencia, que da como resultado la respuesta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "GenerAI Technologies.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(\n",
    "    engine=\"text-davinci-003\",\n",
    "    temperature=0\n",
    ")\n",
    "text = \"Cuál sería un buen nombre para una empresa que haga IA generativa?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trackeando el uso de Tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes utilizar la librería Langchain para trackear el uso de Tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 77\n",
      "\tPrompt Tokens: 10\n",
      "\tCompletion Tokens: 67\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0015400000000000001\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "llm = OpenAI(\n",
    "    engine=\"text-davinci-003\",\n",
    "    n=2,\n",
    "    best_of=2\n",
    ")\n",
    "# Observamos el uso de tokens y su costo total de la API\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Cuentame un chiste sobre IAs\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Few-shot learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El few-shot learning es una capacidad notable que permite a los LLM aprender y generalizar a partir de ejemplos limitados. Las instrucciones sirven de entrada a estos modelos y desempeñan un papel crucial en la consecución de esta característica. Con LangChain, los ejemplos pueden quemarse, pero seleccionarlos dinámicamente a menudo resulta más potente, ya que permite a los LLM adaptarse y abordar rápidamente tareas con un mínimo de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello se utiliza la clase FewShotPromptTemplate, que recibe una PromptTemplate y una lista de ejemplos de disparos. La clase formatea la plantilla con algunos ejemplos de tomas, lo que ayuda al LLM a generar una mejor respuesta. Podemos agilizar este proceso utilizando FewShotPromptTemplate de LangChain para estructurar el enfoque:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# Ejemplos\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Cómo está el clima?\",\n",
    "        \"answer\": \"Están lloviendog atos y perros, mejor lleva sombrilla!\"\n",
    "    }, {\n",
    "        \"query\": \"Cuantos años tienes?\",\n",
    "        \"answer\": \"Los años son los un número, yo no dependo del tiempo.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Creemos el Template de ejemplo\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# Ejemplo de prompt para el template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Separamos el prompt en prefix y sufix, para usarlas como instrucciones\n",
    "prefix = \"\"\"\n",
    "Los siguientes son extractos de conversaciones con un asistente de IA\n",
    "AI. El asistente es conocido por su humor e ingenio, y ofrece\n",
    "respuestas divertidas y entretenidas a las preguntas de los usuarios. He aquí algunos\n",
    "ejemplos:\n",
    "\"\"\"\n",
    "suffix = \"\"\" \n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# Finalmente creamos el few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos la inferencia mediante el Few-Shot learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Esa es una pregunta profunda y filosófica. Pero si me preguntas a mí, diría que el significado de la vida es encontrar la felicidad y hacer del mundo un lugar mejor. Y, por supuesto, tener un buen sentido del humor también ayuda.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain import LLMChain\n",
    "\n",
    "# Modelo\n",
    "chat = ChatOpenAI(\n",
    "    engine=\"gpt-35-turbo\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"Cuál es el significado de la vida?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Habilidades emergentes, leyes de escala y alucinaciones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro aspecto de los LLM son sus capacidades emergentes, que surgen como resultado de un amplio preentrenamiento en vastos conjuntos de datos. Estas capacidades no se programan explícitamente, sino que surgen a medida que el modelo encuentra patrones en los datos. Los modelos LangChain aprovechan estas capacidades emergentes trabajando con varios tipos de modelos, como modelos de chat y modelos de embeddings de texto. Esto permite a los LLM realizar diversas tareas, desde responder preguntas hasta generar texto y ofrecer recomendaciones.\n",
    "\n",
    "Por último, las leyes de escalado describen la relación entre el tamaño del modelo, los datos de entrenamiento y el rendimiento. En general, a medida que aumentan el tamaño del modelo y el volumen de datos de entrenamiento, también lo hace el rendimiento del modelo. Sin embargo, esta mejora está sujeta a rendimientos decrecientes y puede no seguir un patrón lineal. Es esencial tener un equilibrio entre el tamaño del modelo, los datos de entrenamiento, el rendimiento y los recursos empleados en el entrenamiento a la hora de seleccionar y ajustar los LLM para tareas específicas.\n",
    "\n",
    "Los LLMs tienen capacidades notables, pero no están libres de defectos. Una limitación notable es la aparición de alucinaciones, en las que estos modelos producen texto que parece confiable a primera vista, pero que en realidad es incorrecto o no guarda relación con la entrada dada. Además, los LLM pueden presentar sesgos derivados de sus datos de entrenamiento, lo que puede perpetuar estereotipos o generar resultados no deseados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos con Prompts sencillos (multitask): Text Summarization, Text Translation y Question Answering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamando modelos OpenSource de HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "question = \"Cual es la capital de Colombia?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# LLM Hub\n",
    "hub_llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "# Prompt template\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos modificar el prompt template para incluir múltiples preguntas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "\t\"What color is a ripe banana?\\n\"\n",
    ")\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Summarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(engine=\"gpt-3.5-turbo\", temperature=0)\n",
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)\n",
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text = summarization_chain.predict(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)\n",
    "source_language = \"English\"\n",
    "target_language = \"French\"\n",
    "text = \"Your text here\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
