{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entendiendo los Tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenización es un principio fundamental del procesamiento del lenguaje natural (NLP) que desempeña un papel crucial para que los modelos de lenguaje puedan comprender la información escrita. Consiste en descomponer las entradas textuales en unidades individuales denominadas tokens, que constituyen la base para que las redes neuronales puedan comprender y procesar el lenguaje con eficacia. En el notebook anterior, se introdujo el concepto de tokens como medio para definir la entrada de los modelos de lenguaje grandes (LLM). \n",
    "\n",
    "La longitud del contexto del modelo es una característica frecuentemente discutida entre los modelos de lenguaje. Por ejemplo, el modelo GPT-3.5 tiene una longitud de contexto de 4.096 tokens, que abarcan tanto los tokens del prompt como los de la compleción posterior. Debido a esta limitación, es aconsejable tener en cuenta el uso de los tokens al realizar peticiones a los LLMs. No obstante, existen varios métodos para hacer frente a este problema cuando se trata de entradas largas o de varios documentos. Estos métodos incluyen dividir una solicitud larga en segmentos más pequeños (chunks) y enviar las solicitudes secuencialmente o, alternativamente, enviar solicitudes independientes para cada documento y fusionar las respuestas en un paso final.\n",
    "\n",
    "Profundicemos ahora en lo que representan exactamente los tokens y su significado en este contexto. El proceso de tokenización implica la creación de un proceso para transformar las palabras en tokens. Sin embargo, es fundamental comprender claramente qué representan exactamente los tokens en este contexto. Se han incorporado tres enfoques de codificación distintos. La siguiente figura muestra un ejemplo de cada uno de ellos:\n",
    "\n",
    "1. Tokenización a nivel de caracter: considera cada caracter en el texto como un token.\n",
    "\n",
    "2. Tokenización a nivel de palabra: Codifica cada palabra en el corpus como un token.\n",
    "\n",
    "3. A nivel de sub-palabra:  Separa las palabras en chunks cuando es posible.\n",
    "\n",
    "<img src=\"images/tokenization.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificación a nivel de subpalabra ofrece una mayor flexibilidad y reduce el número de tokens únicos necesarios para representar un corpus. Este planteamiento permite combinar distintos tokens para representar palabras nuevas, eliminando la necesidad de añadir cada palabra nueva al diccionario. Esta técnica ha demostrado ser la codificación más eficaz a la hora de entrenar redes neuronales y LLM. Modelos bien conocidos como la familia GPT o LLaMA emplean este método de tokenización. Por lo tanto, nos centraremos exclusivamente en una de sus variantes específicas, conocida como codificación por pares de bytes (BPE). Cabe mencionar que existen otros algoritmos a nivel de subpalabra, como WordPiece y SentencePiece, que se utilizan en la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Byte Pair Encoding (BPE)**\n",
    "\n",
    "Se trata de un proceso iterativo para extraer las palabras o subpalabras más repetitivas de un corpus. El algoritmo comienza contando las apariciones de cada carácter y se basa en la fusión de caracteres. Es un proceso codicioso que considera cuidadosamente todas las combinaciones posibles para identificar el conjunto óptimo de palabras/subpalabras que cubre el conjunto de datos con el menor número de tokens necesarios.\n",
    "\n",
    "El siguiente paso consiste en crear el vocabulario para nuestro modelo, que consiste en un diccionario exhaustivo compuesto por los tokens más frecuentes extraídos mediante BPE (u otra técnica de su elección) del conjunto de datos. La definición de diccionario (tipo dict) es una estructura de datos que contiene un par de clave y valor para cada fila. En nuestro caso, a cada punto de datos se le asigna una clave representada por un índice que empieza por 0, mientras que el valor correspondiente es un token.\n",
    "\n",
    "Dado que las redes neuronales sólo aceptan entradas numéricas, podemos utilizar el vocabulario para establecer una correspondencia entre los tokens y sus ID correspondientes, como una tabla de búsqueda. Tenemos que guardar el vocabulario para futuros casos de uso para poder descodificar la salida del modelo de los ID a palabras. Esto se conoce como vocabulario preentrenado, un componente esencial que acompaña a los modelos preentrenados. Sin el vocabulario, sería imposible entender el resultado del modelo (los ID). En el caso de los modelos más pequeños, como BERT, el diccionario puede constar de tan sólo 30.000 tokens, mientras que los modelos más grandes, como GPT-3, pueden llegar a abarcar hasta 50.000 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokens y sus costos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe una correlación directa entre los tokens y el coste en la mayoría de las API propietarias como OpenAI. Es crucial destacar que los precios se dividen en dos categorías: el número de tokens en la solicitud y la completion (la generación del modelo), siendo la completion la que suele incurrir en costes más elevados. Por ejemplo, en el momento de escribir esta lección, GPT-4 costará 0,03$ por 1K tokens para procesar sus entradas, y 0,06$ por 1K tokens para el proceso de generación. Como vimos en el notebook anterior, puedes usar el método get_openai_callback de LangChain para obtener el coste exacto, o hacer el proceso de tokenización localmente y llevar la cuenta del número de tokens como veremos en la siguiente sección. Para una estimación aproximada, como regla general, OpenAI considera que 1K tokens equivalen aproximadamente a 750 palabras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
