{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables de entorno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_type = os.environ[\"OPENAI_API_TYPE\"]\n",
    "openai.api_version = os.environ[\"OPENAI_API_VERSION\"]\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LLMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El componente fundamental de LangChain implica invocar un LLM con un input específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro de temperatura en los modelos OpenAI gestiona la aleatoriedad de la salida. Cuando se ajusta a 0, la salida está predeterminada en su mayor parte y es adecuada para tareas que requieren estabilidad y el resultado más probable. Con un valor de 1,0, el resultado puede ser incoherente e interesante, pero no es aconsejable para la mayoría de las tareas. Para tareas creativas, una temperatura entre 0,70 y 0,90 ofrece un equilibrio entre fiabilidad y creatividad. El mejor ajuste debe determinarse experimentando con diferentes valores para cada caso de uso específico. El código inicializa la variante Davinci del modelo GPT-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(\n",
    "    engine=\"text-davinci-003\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Empieza con una comprensión general de la Inteligencia Artificial (IA). Esto incluye aprender sobre los algoritmos básicos de IA, como los algoritmos de aprendizaje profundo, redes neuronales y máquinas de soporte vectorial.\n",
      "\n",
      "2. Familiarícese con los conceptos de aprendizaje automático (ML), como regresión lineal, árboles de decisión, regresión logística y clustering.\n",
      "\n",
      "3. Profundiza en los conceptos de redes neuronales profundas (DNNs), como arquitecturas de redes neuronales, convolucionales y recurrentes.\n",
      "\n",
      "4. Aprende sobre los diferentes tipos de aprendizaje profundo, como el aprendizaje profundo supervisado, no supervisado y reforzado.\n",
      "\n",
      "5. Aprende sobre los diferentes tipos de modelos de aprendizaje profundo, como redes\n"
     ]
    }
   ],
   "source": [
    "text_input = \"\"\"Sugiere una ruta de aprendizaje personalizada para alguien\n",
    "que busca aprender IA y especializarse en LLMs\n",
    "\"\"\"\n",
    "print(llm(text_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Chains**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En LangChain, una cadena es un wrapper de extremo a extremo alrededor de múltiples componentes individuales, proporcionando una forma de lograr un caso de uso común mediante la combinación de estos componentes en una secuencia específica. El tipo de cadena más utilizado es LLMChain, que consiste en un PromptTemplate, un modelo (ya sea un LLM o un ChatModel) y un analizador de salida opcional.\n",
    "\n",
    "La LLMChain funciona de la siguiente forma:\n",
    "\n",
    "- Recibe múltiples variables de entrada.\n",
    "- Utiliza un PromptTemplate para darle formato a las variables de entrada en un prompt específico.\n",
    "- Pasa el prompt formateado al modelo de lenguaje (LLM o ChatModel)\n",
    "- Si se proporciona un analizador de salida, se usa el OutputParser para analizar la salida del LLM en un formato final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El nombre sería: ?\n",
      "\n",
      "Pythonic Code Generator.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(\n",
    "    engine=\"text-davinci-003\",\n",
    "    temperature=0.85)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"action\"],\n",
    "    template=\"\"\"Cuál es un buen nombre para un modelo de lenguaje \n",
    "            que hace {action}\"\"\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt) # Instanciamos la chain\n",
    "print(f\"El nombre sería: {chain.run('código en Python')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En LangChain, la memoria se refiere a la funcionalidad que almacena y gestiona el historial de conversaciones entre un usuario y la IA. Ayuda a mantener el contexto y la coherencia a lo largo de la interacción, permitiendo a la IA generar respuestas más relevantes y precisas. La memoria, como la clase ConversationBufferMemory, actúa como un wrapper alrededor de ChatMessageHistory, extrayendo los mensajes y proporcionándolos a la cadena (Chain) para una mejor generación consciente del contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hablame sobre IA generativa.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hablame sobre IA generativa.\n",
      "AI:  Generative AI es una forma de inteligencia artificial que se centra en la creación de contenido nuevo y original. Esto se logra mediante el uso de algoritmos de aprendizaje profundo para generar contenido que se asemeja a contenido creado por humanos. Esto se puede aplicar a muchas áreas, como la creación de imágenes, música, texto y videos.\n",
      "Human: Puedes generar codigo?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hablame sobre IA generativa.\n",
      "AI:  Generative AI es una forma de inteligencia artificial que se centra en la creación de contenido nuevo y original. Esto se logra mediante el uso de algoritmos de aprendizaje profundo para generar contenido que se asemeja a contenido creado por humanos. Esto se puede aplicar a muchas áreas, como la creación de imágenes, música, texto y videos.\n",
      "Human: Puedes generar codigo?\n",
      "AI:  Sí, la IA generativa también se puede usar para generar código. Esto se logra mediante el uso de algoritmos de aprendizaje profundo para generar código que se asemeja al código creado por humanos. Esto se puede aplicar a muchas áreas, como la creación de aplicaciones web, aplicaciones móviles y software de escritorio.\n",
      "Human: Podrias ayudarme a construir una IA?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationChain(memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hablame sobre IA generativa.', additional_kwargs={}, example=False), AIMessage(content=' Generative AI es una forma de inteligencia artificial que se centra en la creación de contenido nuevo y original. Esto se logra mediante el uso de algoritmos de aprendizaje profundo para generar contenido que se asemeja a contenido creado por humanos. Esto se puede aplicar a muchas áreas, como la creación de imágenes, música, texto y videos.', additional_kwargs={}, example=False), HumanMessage(content='Puedes generar codigo?', additional_kwargs={}, example=False), AIMessage(content=' Sí, la IA generativa también se puede usar para generar código. Esto se logra mediante el uso de algoritmos de aprendizaje profundo para generar código que se asemeja al código creado por humanos. Esto se puede aplicar a muchas áreas, como la creación de aplicaciones web, aplicaciones móviles y software de escritorio.', additional_kwargs={}, example=False), HumanMessage(content='Podrias ayudarme a construir una IA?', additional_kwargs={}, example=False), AIMessage(content=' Sí, puedo ayudarte a construir una IA. Primero, necesitarás definir los objetivos de tu IA. Esto te ayudará a determinar qué tipo de algoritmos de aprendizaje profundo necesitarás para lograr tus objetivos. Luego, necesitarás recopilar y preparar los datos necesarios para entrenar tu IA. Finalmente, necesitarás entrenar tu IA y evaluar su rendimiento.', additional_kwargs={}, example=False)]), output_key=None, input_key=None, return_messages=False, human_prefix='Human', ai_prefix='AI', memory_key='history'), callbacks=None, callback_manager=None, verbose=True, tags=None, prompt=PromptTemplate(input_variables=['history', 'input'], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:', template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={'engine': 'text-davinci-003'}, openai_api_key='016def5f97c24e9588b9e276f557c140', openai_api_base='https://pocdevops-dev-oai-01.openai.azure.com/', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='response', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}, input_key='input')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(\n",
    "    engine=\"text-davinci-003\",\n",
    "    temperature=0\n",
    ")\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "# Comenzamos la conversación\n",
    "conversation.predict(input=\"Hablame sobre IA generativa.\")\n",
    "\n",
    "# Continuamos la conversación\n",
    "conversation.predict(input=\"Puedes generar codigo?\")\n",
    "conversation.predict(input=\"Podrias ayudarme a construir una IA?\")\n",
    "\n",
    "# Veamos la conversación\n",
    "conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deep Lake VectorStore**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Lake proporciona almacenamiento para embeddings y sus metadatos correspondientes en el contexto de aplicaciones LLM. Permite realizar búsquedas híbridas en estos embeddings y sus atributos para una recuperación eficaz de los datos. También se integra con LangChain, facilitando el desarrollo y despliegue de aplicaciones.\n",
    "\n",
    "Deep Lake ofrece varias ventajas sobre el típico almacén vectorial:\n",
    "\n",
    "- Es multimodal, lo que significa que puede utilizarse para almacenar elementos de diversas modalidades, como textos, imágenes, audio y vídeo, junto con sus representaciones vectoriales. \n",
    "- Es serverless, lo que significa que podemos crear y gestionar conjuntos de datos en la nube sin necesidad de crear y gestionar una instancia de base de datos. Este aspecto aporta una gran aceleración a los nuevos proyectos.\n",
    "- Por último, es posible crear fácilmente un cargador de datos a partir de los datos cargados en un conjunto de datos de Deep Lake. Resulta práctico para ajustar modelos de aprendizaje automático utilizando frameworks populares como PyTorch y TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ACTIVELOOP_TOKEN\"] = \"eyJhbGciOiJIUzUxMiIsImlhdCI6MTY4OTk1MDE5MiwiZXhwIjoxNjkwMTIyOTU4fQ.eyJpZCI6ImpwYWd1aWxhcmM5OSJ9.n8wkzYiJ_nI64A5NTfQBVnJd-n9UK6Cd-WheCXeQ_ECFs_eNdXOfZzLPQYjJ84o8T2CeSLJoRjA8m7Tp7EqfsA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación deeplake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deeplake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dotamos al LLM de información externa a su conocimiento y utilizamos embeddings para calcular documentos similares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://jpaguilarc99/langchain_hackathon_llm_example already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://jpaguilarc99/langchain_hackathon_llm_example', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['7ddde2d7-28cb-11ee-b8f9-fc3497bf337a',\n",
       " '7ddde2d8-28cb-11ee-9aea-fc3497bf337a']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Antes de ejecutar el siguiente código, asegúrese de tener su\n",
    "# clave de Activeloop guardada en la variable de entorno \"ACTIVELOOP_TOKEN\".\n",
    "\n",
    "# Instanciamos el LLM y el modelo de embeddings (text-embedding-ada-002 de OpenAI)\n",
    "llm = OpenAI(\n",
    "    engine=\"text-davinci-003\",\n",
    "    temperature=0\n",
    ")\n",
    "embeddings = OpenAIEmbeddings() # model=\"text-embedding-ada-002\"\n",
    "\n",
    "# Creamos documentos dummies para hacer nuestra base de conocimientos\n",
    "texts = [\n",
    "    \"La Hackathon de LLMs de Bancolombia es un evento para impulsar el desarrollo de aplicaciones con LLMs\",\n",
    "    \"Los equipos de la Hackathon de LLMs son conformados por máximo 4 personas y se asignará un reto\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# Creamos el Deep Lake dataset\n",
    "# Debemos insertar id organization (username de deeplake por default)\n",
    "my_activeloop_org_id = \"jpaguilarc99\"\n",
    "my_activeloop_dataset_name = \"langchain_hackathon_llm_example\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# Agregamos nuestros documentos (texts) en forma de embeddings al dataset de DeepLake\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RetrievalQA chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agente que utiliza a RetrievalQA chain como una herramienta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out what a Hackathon de LLMs is.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is a Hackathon de LLMs?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m A Hackathon de LLMs is an event to promote the development of applications using large language models (LLMs). Teams of up to four people are assigned a challenge and use AI generative technology to create original and realistic content such as text, images, music, or video.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: A Hackathon de LLMs is an event to promote the development of applications using large language models (LLMs). Teams of up to four people are assigned a challenge and use AI generative technology to create original and realistic content such as text, images, music, or video.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "A Hackathon de LLMs is an event to promote the development of applications using large language models (LLMs). Teams of up to four people are assigned a challenge and use AI generative technology to create original and realistic content such as text, images, music, or video.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent.run(\"Qué es la Hackathon de LLMs?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí, el agente ha utilizado la herramienta \"Retrieval QA System\" con la consulta \"¿Qué es la Hackathon de LLMs?\", que se ejecuta en nuestro nuevo conjunto de datos Deep Lake, devolviendo el documento más similar (es decir, el documento que contiene la definición de la Hackathon de LLMs). Este documento se utiliza finalmente para generar el resultado final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero recargamos un vector store existente de Deep Lake que se encuentra en una ruta de conjunto de datos especificada. A continuación, cargamos los nuevos datos textuales y los dividimos en fragmentos o chunks. Por último, añadimos estos fragmentos al conjunto de datos existente, creando y almacenando los embeddings correspondientes para cada segmento de texto añadido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://jpaguilarc99/langchain_hackathon_llm_example already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://jpaguilarc99/langchain_hackathon_llm_example', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (6, 1536)  float32   None   \n",
      "    id        text      (6, 1)      str     None   \n",
      " metadata     json      (6, 1)      str     None   \n",
      "   text       text      (6, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['6cb7d3a3-28cc-11ee-9092-fc3497bf337a',\n",
       " '6cb7d3a4-28cc-11ee-bdca-fc3497bf337a']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargamos el dataset existente de Deep Lake e instanciamos el modelo de embeddings\n",
    "db = DeepLake(\n",
    "    dataset_path=dataset_path,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Creamos nuevos documentos\n",
    "texts = [\n",
    "    \"Mi gata es naranja y se llama La Gorda\",\n",
    "    \"Sebastian nació en los años 80\"\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.create_documents(texts)\n",
    "\n",
    "# Agregamos los documentos al dataset de DeepLake\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a crear el Agent y preguntamos algo que solo se puede responder con los últimos documentos añadidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the name of the orange cat.\n",
      "Action: Retrieval QA System\n",
      "Action Input: What is the name of the orange cat?\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m La Gorda\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: La Gorda\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "La Gorda\n"
     ]
    }
   ],
   "source": [
    "# Instanciamos el LLM\n",
    "llm = OpenAI(engine=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Creamos el Document Retriever\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "\tllm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# Instanciamos las tools para que use el retriever\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Retrieval QA System\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Useful for answering questions.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Creamos el agente que utiliza las Tools\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = agent.run(\"Cómo se llama la gata naranja?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El LLM recupera con éxito información precisa utilizando Deep Lake como Vector Storage y el modelo de lenguaje de OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Agentes en LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En LangChain, los agentes son componentes de alto nivel que utilizan modelos de lenguaje (LLMs) para determinar qué acciones realizar y en qué orden. Una acción puede ser utilizar una herramienta y observar su resultado o devolvérselo al usuario. Las herramientas (tools) son funciones que realizan tareas específicas, como Google Search, búsquedas en bases de datos o Python REPL.\n",
    "\n",
    "Los Agentes implican que un LLM tome decisiones sobre qué Acciones realizar, realice esa Acción, vea una Observación y repita eso hasta que termine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchos tipos de agentes están disponibles en LangChain:\n",
    "\n",
    "- El agente \"zero-shot-react-description\" utiliza el marco ReAct para decidir qué herramienta emplear basándose únicamente en la descripción de la misma. Necesita una descripción de cada herramienta.\n",
    "- El agente \"react-docstore\" se relaciona con un docstore a través del marco ReAct. Necesita dos herramientas: una de búsqueda y otra de consulta. La herramienta de búsqueda encuentra un documento y la herramienta de búsqueda busca un término en el último documento encontrado.\n",
    "- El agente de \"self-ask-with-search\" emplea una única herramienta denominada Respuesta intermedia, que es capaz de buscar respuestas factuales a las consultas. Es idéntica a la autopregunta original con el agente de búsqueda, en la que se proporcionaba como herramienta una API de búsqueda de Google.\n",
    "- El agente \"conversational-react-description\" está diseñado para situaciones conversacionales. Utiliza el marco ReAct para seleccionar una herramienta y emplea la memoria para recordar interacciones conversacionales anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el siguiente ejemplo de Google Search se necesitarán los tokens de Google: “GOOGLE_API_KEY” y “GOOGLE_CSE_ID”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "# Dotamos el LLM de acceso al buscador de Google\n",
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "# Creamos la Tool\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"google-search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to search google to answer questions about current events\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Creamos el agente\n",
    "agent = initialize_agent(tools, \n",
    "                         llm, \n",
    "                         agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose=True,\n",
    "                         max_iterations=6)\n",
    "\n",
    "response = agent(\"What's the latest news about the Mars rover?\")\n",
    "print(response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tools en LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain proporciona una variedad de herramientas para que los agentes interactúen con el mundo exterior. Estas herramientas se pueden utilizar para crear agentes personalizados que realicen diversas tareas, como buscar en la web, responder preguntas o ejecutar código Python. En esta sección, discutiremos los diferentes tipos de herramientas disponibles en LangChain y proporcionaremos ejemplos de su creación y uso.\n",
    "\n",
    "En nuestro ejemplo, dos herramientas están siendo definidas para su uso dentro de un agente LangChain: una herramienta de Búsqueda de Google y una herramienta de Modelo de Lenguaje que actúa específicamente como un resumidor de texto. La herramienta de búsqueda de Google, que utiliza el GoogleSearchAPIWrapper, gestionará las consultas que impliquen la búsqueda de información sobre eventos recientes. La herramienta de modelo de lenguaje aprovecha las capacidades de un LLM para resumir textos. Estas herramientas están diseñadas para ser utilizadas indistintamente por el agente, dependiendo de la naturaleza de la consulta del usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "llm = OpenAI(engine=\"text-davinci-003\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Escribe un resumen para el siguiente texto: {query}\"\n",
    ")\n",
    "summarize_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for finding information about recent events\"\n",
    "    ),\n",
    "    Tool(\n",
    "       name='Summarizer',\n",
    "       func=summarize_chain.run,\n",
    "       description='useful for summarizing texts'\n",
    "    ),\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True  \n",
    ")\n",
    "\n",
    "response = agent(\"¿Cuáles son las últimas noticias sobre el explorador de Marte? A continuación, resuma los resultados.\")\n",
    "print(response['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
